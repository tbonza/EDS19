---
title: "A1: Data Acquisition"
author: Tyler Brown
output: pdf_document
bibliography: references.bib
---
```{r, include=FALSE}
library(tibble)
library(knitr)
```

The approach I used to complete Assignment 1 was to create a Python
package, Okra, which comes with a command line tool to automate process.

# Architecture

## Using Okra

A command line tool is provided upon installing Okra for Assignment 1.
Below is the help menu for this tool.

```{r, eval=FALSE}
(okra) bash-3.2$ assn1 -h
usage: assn1 [-h] action repo_list directory_path logname

positional arguments:
  action          'get', 'update', 'extract_data' for git repos
  repo_list       file path to list of GitHub repos
  directory_path  file path to directory storing git repos
  logname         name of log file

optional arguments:
  -h, --help      show this help message and exit
```

We can see that three actions are available: 'get', 'update', and
'extract_data'. The 'get' action will perform `git clone` on all 
repositories in the "repos.list" input file we were assigned. The
'update' action will perform `git fetch` on all these repos. I didn't
want to try to merge anything because upstream commits can require
manual merging; we want this process to be automated. A repo is also not
required to use `master` as it's main branch even though it would be the
conventional practice. The 'extract\_data` action will generate the three
csv files which were requested for Assignment 1. 

## Deploying Okra

Okra stores all git repositories from `repo\_list` in `directory\_path`. 
In the case of Assignment 1, we're storing about 98GB of data. I used an
AWS EC2 instance with 4GB of memory to process all of the repos. The repos
are saved on an 150GB AWS EBS volume. This approach allowed me to bring
down the EC2 instance when I'm not using it while persisting data in the
EBS volume. I like the approach because I do not have to keep a compute
server running. Gotchas for this approach are that the EBS volume and
EC2 instance must share the same subnet in addition to the same region. Be
sure to appropriately set the subnet if you're going to use this setup.

An IT Automation tool is used to automate some provisioning tasks for
the EC2 instance [@ansible]. The Ansible playbook for this assignment
is located under the `configuration` directory outside of the `01-data-acqu`
directory. Ansible basically uses yaml configuration files to complete
tasks after connecting to a server with SSH.

## Unsuccessful Approaches

I attempted a number of unsuccessful approaches involving Data Pipeline
Frameworks for this assignment. I found the frameworks to be overkill for
this assignment. They're generally expensive to run, complicated to set
up, and somewhat dependent on specific programming languages. The following
list details what I tried and why it didn't work for this assignment.

* Apache Airflow [@airflow]
  - Too many dependencies required to quickly launch application
  - Does not currently support Python 3.7, needs Python 3.6
  - Requires message broker like RabbitMQ to run locally or distributed
	which is more set up and dependencies
* Apache Nifi [@nifi]
  - Uses own domain specific language (DSL) to manipulate data
  - Very dependent on Java
  - Too expensive and complicated to set up for the scope of this assignment
* Spring Cloud Data Flow [@pivotal]
  - Too expensive to run and difficult to set up
  - Can use shell scripting for streams but tasks must be done
	in Java using the Spring Framework
	
If given more time, I'll continue using the Python package, "okra", that
I wrote. It seems more important to focus on the analysis rather than the
tools at this stage.

# Performance

The internet connection speeds for an AWS instance probably helped 
performance quite a bit. Using a programming language besides Python
would have most likely sped up data extraction. 

## Run Time

```{r, echo=FALSE}
runtime <- tibble(
	`Process` = c("Get repos", "Update repos", "Extract data"),
	`Log Start Time` = c("2019-01-21 02:54:16", "2019-01-22 03:08:06",
	"2019-01-22 07:31:02"),
	`Log End Time` = c("2019-01-21 04:40:18", "2019-01-22 03:11:34",
	"2019-01-22 09:36:33"),
	`Hours Estimate` = c(1.5, 0.05, 2.0)
)
kable(runtime, caption="Run Times by Process")
```

Table 1 shows us that the data collection from start to finish would
be about 3.5 hours if the git repositories were not already stored. If
the git repos are being stored then data collection from start to finish
would be about 2.5 hours. These run times are well within the 24 hour 
maximum run time specified in Assignment 1. 

## Disk Space

```{r, echo=FALSE}
disk <- tibble(
`Name` = c("commits.csv", "files.csv", "messages.csv", "/repos/"),
`Disk usage` = c("881M", "1.9G", "1.3G", "98G")
)
kable(disk, caption="Disk Usage by Name")
```

Data collection is efficient but it takes up about 102GB of memory. This
seems fine given that we'll need to repeatedly reference this dataset
throughout the semester.

# Discussion

My approach for Assignment 1 was to create a Python package, Okra, to
automate a bunch of shell commands, provide logging, and a command line
interface. By deploying Okra on AWS, I was able to take advantage of good
network speeds and collect data well under the maximum time limit. Data 
pipelines are overkill and take away from time which could be spent on
the analysis at this stage of the project. I'll most likely continue adding
to the Okra package for Assignment 2 unless requested to do otherwise.

# References

