{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Need to figure out how to generate parquet files for Spark\n",
    "from the SQLite databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from okra.models import DataAccessLayer\n",
    "from okra.logging_utils import enable_cloud_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cloud_log()\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['torvalds__REPODB__linux.db',\n",
       " 'apache__REPODB__attic-wink.db',\n",
       " 'apache__REPODB__attic-lucy.db',\n",
       " 'docker__REPODB__docker-ce.db',\n",
       " 'apache__REPODB__lucene-solr.db',\n",
       " 'apache__REPODB__spark.db']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_dir = '/Users/tylerbrown/code/'\n",
    "dbs = [i for i in os.listdir(db_dir) if i[-3:] == \".db\"]\n",
    "dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet_table(table_name: str, query: str, db_dir: str, dbs: list, batch_size):\n",
    "    logger.info(\"Writing parquet files for {}\".format(table_name))\n",
    "    df = None\n",
    "    first = True\n",
    "    datenow = datetime.now().strftime('%Y-%m-%d')\n",
    "    table_format = table_name + \"_{}_{}.parquet\"\n",
    "    \n",
    "    subnum = 0\n",
    "    for db in dbs:\n",
    "        \n",
    "        conn = 'sqlite:///' + db_dir + db\n",
    "        \n",
    "        if first:\n",
    "            df = pd.read_sql(query, conn)\n",
    "            first = False\n",
    "        else:\n",
    "            da = pd.read_sql(query, conn)\n",
    "            df = df.append(da)\n",
    "            \n",
    "        if df.shape[0] > batch_size:\n",
    "            \n",
    "            table_name = table_format.format(datenow, subnum)\n",
    "            table_path = urljoin(db_dir, table_name)\n",
    "            df_out = df.iloc[0:batch_size]\n",
    "            table = pa.Table.from_pandas(df_out)\n",
    "            pq.write_table(table, table_path)\n",
    "            logger.info(\"Wrote parquet file: {}\".format(table_path))\n",
    "            subnum += 1\n",
    "            \n",
    "            df = df.iloc[batch_size:]\n",
    "            \n",
    "    if df.shape[0] > 0:\n",
    "        \n",
    "        table_name = table_format.format(datenow, subnum)\n",
    "        table_path = urljoin(db_dir, table_name)\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, table_path)\n",
    "        logger.info(\"Wrote parquet file: {}\".format(table_path))\n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 14:17:58,870 INFO Writing parquet files for commit_file\n",
      "/Users/tylerbrown/miniconda3/envs/rose/lib/python3.6/site-packages/pyarrow/pandas_compat.py:114: FutureWarning: A future version of pandas will default to `skipna=True`. To silence this warning, pass `skipna=True|False` explicitly.\n",
      "  result = infer_dtype(pandas_collection)\n",
      "2019-04-20 14:18:07,736 INFO Wrote parquet file: /Users/tylerbrown/code/commit_file_2019-04-20_0.parquet\n",
      "2019-04-20 14:18:08,513 INFO Wrote parquet file: /Users/tylerbrown/code/commit_file_2019-04-20_1.parquet\n"
     ]
    }
   ],
   "source": [
    "table_name = 'commit_file'\n",
    "query = 'SELECT * FROM commit_file;'\n",
    "db_dir = '/Users/tylerbrown/code/'\n",
    "\n",
    "write_parquet_table(table_name, query, db_dir, dbs, batch_size=int(2e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite_to_parquet(tables: list, db_dir, batch_size):\n",
    "    \n",
    "    logger.info(\"STARTED converting all sqlite databases to parquet\")\n",
    "    dbs = [i for i in os.listdir(db_dir) if i[-3:] == \".db\"]\n",
    "    query_format = \"SELECT * FROM {};\"\n",
    "    for table_name in tables:\n",
    "       \n",
    "        query = query_format.format(table_name)\n",
    "        logger.info(\"Created query: {}\".format(query))\n",
    "        write_parquet_table(table_name, query, db_dir, dbs, batch_size)\n",
    "        \n",
    "    logger.info(\"FINISHED converting all sqlite databases to parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 14:18:08,709 INFO STARTED converting all sqlite databases to parquet\n",
      "2019-04-20 14:18:08,710 INFO Created query: SELECT * FROM meta;\n",
      "2019-04-20 14:18:08,711 INFO Writing parquet files for meta\n",
      "2019-04-20 14:18:11,951 INFO Wrote parquet file: /Users/tylerbrown/code/meta_2019-04-20_0.parquet\n",
      "2019-04-20 14:18:12,016 INFO Created query: SELECT * FROM author;\n",
      "2019-04-20 14:18:12,016 INFO Writing parquet files for author\n",
      "2019-04-20 14:18:23,694 INFO Wrote parquet file: /Users/tylerbrown/code/author_2019-04-20_0.parquet\n",
      "2019-04-20 14:18:23,817 INFO Created query: SELECT * FROM contrib;\n",
      "2019-04-20 14:18:23,818 INFO Writing parquet files for contrib\n",
      "2019-04-20 14:18:37,129 INFO Wrote parquet file: /Users/tylerbrown/code/contrib_2019-04-20_0.parquet\n",
      "2019-04-20 14:18:37,248 INFO Created query: SELECT * FROM commit_file;\n",
      "2019-04-20 14:18:37,249 INFO Writing parquet files for commit_file\n",
      "2019-04-20 14:18:46,426 INFO Wrote parquet file: /Users/tylerbrown/code/commit_file_2019-04-20_0.parquet\n",
      "2019-04-20 14:18:47,233 INFO Wrote parquet file: /Users/tylerbrown/code/commit_file_2019-04-20_1.parquet\n",
      "2019-04-20 14:18:47,418 INFO Created query: SELECT * FROM info;\n",
      "2019-04-20 14:18:47,418 INFO Writing parquet files for info\n",
      "2019-04-20 14:19:04,373 INFO Wrote parquet file: /Users/tylerbrown/code/info_2019-04-20_0.parquet\n",
      "2019-04-20 14:19:04,962 INFO FINISHED converting all sqlite databases to parquet\n"
     ]
    }
   ],
   "source": [
    "tables = ['meta','author','contrib', 'commit_file', 'info']\n",
    "db_dir = '/Users/tylerbrown/code/'\n",
    "batch_size = int(2e6)\n",
    "sqlite_to_parquet(tables, db_dir, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
